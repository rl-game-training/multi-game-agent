{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"initiate_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQBhUVcAKfyR0HljrrgjrY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hq0ad6tQan8J","executionInfo":{"status":"ok","timestamp":1610555741134,"user_tz":-120,"elapsed":6950,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"a1590546-38db-4ef2-dab7-3583d19a81ba"},"source":["import os\n","from getpass import getpass\n","user = getpass('GitHub user')\n","password = getpass('GitHub password')\n","os.environ['GITHUB_AUTH'] = user + ':' + password\n","!git clone https://$GITHUB_AUTH@github.com/rl-game-training/multi-game-agent"],"execution_count":1,"outputs":[{"output_type":"stream","text":["GitHub user··········\n","GitHub password··········\n","fatal: destination path 'multi-game-agent' already exists and is not an empty directory.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HfhVhZyccli_","executionInfo":{"status":"ok","timestamp":1610555741135,"user_tz":-120,"elapsed":6934,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"796dbf8d-8ccf-4171-ce3e-8fa665f05b4f"},"source":["cd multi-game-agent/src"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/multi-game-agent/src\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eWlfQ2c3cvNb","executionInfo":{"status":"ok","timestamp":1610555741365,"user_tz":-120,"elapsed":7153,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["!rm best_net reward_history\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsESr6TWcyZu","executionInfo":{"status":"ok","timestamp":1610555741616,"user_tz":-120,"elapsed":7394,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"a0b1eed8-051a-4770-e841-2728b3199ac5"},"source":["!git pull origin vanilla-dqn\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["From https://github.com/rl-game-training/multi-game-agent\n"," * branch            vanilla-dqn -> FETCH_HEAD\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eny1oFLXc7SO","executionInfo":{"status":"ok","timestamp":1610555741617,"user_tz":-120,"elapsed":7385,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"3f436064-6a30-49be-85e1-06c1dbec2dc4"},"source":["!git checkout vanilla-dqn"],"execution_count":5,"outputs":[{"output_type":"stream","text":["D\tsrc/best_net\n","D\tsrc/reward_history\n","Already on 'vanilla-dqn'\n","Your branch is up to date with 'origin/vanilla-dqn'.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxxJWU_pc_Xz","executionInfo":{"status":"ok","timestamp":1610555742330,"user_tz":-120,"elapsed":8089,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"d7ece141-223e-42fc-83f0-5a20553b8cc4"},"source":["ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["InitialNotebook.ipynb  network.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  train_assault.py  \u001b[01;34mvideo\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8-AxnvAVyzQQ","executionInfo":{"status":"ok","timestamp":1610555745233,"user_tz":-120,"elapsed":10983,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"TCelFzWY9MBI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610555755723,"user_tz":-120,"elapsed":21464,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"fa703e8b-97a9-48ca-9ffb-c92dd8b50dce"},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[atari] > /dev/null 2>&1"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (51.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pdb2JwZy4jGj","executionInfo":{"status":"ok","timestamp":1610555757347,"user_tz":-120,"elapsed":23078,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from IPython import display as ipythondisplay"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQEtc28G4niA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610555757349,"user_tz":-120,"elapsed":23070,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"7dc333af-5e64-4cf7-bfda-c83cb43c033e"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f4de703b550>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"G9UWeToN4r7D","executionInfo":{"status":"ok","timestamp":1610555757350,"user_tz":-120,"elapsed":23063,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVg9D4cJdDhI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610555762054,"user_tz":-120,"elapsed":27759,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"478f570c-8942-4bb6-cca2-aee704d00f1b"},"source":["import train_assault"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Box(0, 255, (210, 160, 3), uint8)\n","Discrete(6)\n","3\n","['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vx6Qkhf5lkow","executionInfo":{"status":"ok","timestamp":1610555762057,"user_tz":-120,"elapsed":27753,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["train_assault.REPLAY_BUFFER_LEN = 700000\n","train_assault.TRANSITIONS_BATCH_SIZE = 128"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"JIHSwTwndTDW","executionInfo":{"status":"ok","timestamp":1610555762058,"user_tz":-120,"elapsed":27747,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["train_assault.env = wrap_env(train_assault.env)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"g30MOCI3lViq","executionInfo":{"status":"error","timestamp":1610560769822,"user_tz":-120,"elapsed":5035508,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}},"outputId":"06c4354b-2516-40dd-98c9-099d6d34e04d"},"source":["train_assault.iterate_train(20000)\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["episode done, reward:  -20.0\n","loss:  tensor(94.8084, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(104.9140, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(87.1023, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(54.6148, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(54.8542, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(59.8975, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(67.2548, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(63.9970, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(67.5212, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(89.7349, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(81.2963, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(72.9248, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(63.9449, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(67.4985, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(87.8514, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(111.3981, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(107.3226, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(105.0294, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(130.3477, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(119.9555, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(135.6943, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(151.9135, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(132.1229, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(158.3775, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(157.4079, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(155.6750, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(152.1151, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(173.7796, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(205.5521, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(186.3739, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(168.7862, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(210.3470, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(254.4346, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(230.9670, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(284.1093, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(292.7628, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(262.5290, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(323.2473, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(380.7710, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(346.9597, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(417.7936, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(458.6268, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(567.8276, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(499.6319, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(574.0529, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(764.7315, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(936.6757, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(843.1279, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(840.7932, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(999.8578, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(930.9707, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(1150.2842, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1030.6787, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1101.2378, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(997.0725, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1185.1417, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(1377.1803, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1322.1879, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1242.6486, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1358.9072, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1357.3362, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1449.5890, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1766.5573, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1797.6149, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1656.3060, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1667.4733, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(1716.9424, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1876.3317, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1870.6106, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1726.0432, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1798.7126, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1947.4766, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1570.5775, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(1771.7518, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1936.8199, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1746.6727, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(1990.8419, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2008.4067, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(1929.7567, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2159.9216, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2039.9688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2133.3782, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2509.4556, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(2959.7939, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(3083.2432, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(2729.3999, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(3659.8640, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(2984.7920, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(3210.5305, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3267.3481, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3912.1570, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3811.2686, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3481.7620, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4423.0005, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4075.2063, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3838.4773, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3819.6934, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(3928.3440, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4319.6509, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4602.2134, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4168.0615, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4750.6172, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4555.2197, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4313.4521, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(5021.6802, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(5003.1895, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4815.5381, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4592.8955, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(5467.6040, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(5568., device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(4907.2778, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(6092.8237, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(5651.4805, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(6020.4795, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(6678.5967, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(7081.5791, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(6764.7129, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(7127.9629, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9544.4854, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9734.1162, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9325.6182, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9783.7520, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(8780.3818, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(7247.4595, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(7251.5244, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9183.7676, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9557.6455, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9430.8408, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(8990.3594, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(8588.4102, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(10114.3652, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(10700.5635, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(10026.0439, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(9921.2363, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(11243.7871, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(14621.6719, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(14622.7031, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(11968.4121, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(12578.5781, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(14991.4424, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(13782.8516, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(15831.7207, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(16230.0303, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(19055.2773, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(17459.0723, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(19718.4609, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(18194.4336, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(17061.1309, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(17702.5938, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(21651.8398, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(23299.6895, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(21123.6855, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(22213.2500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(21503.2754, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(24364.0527, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(22433.8789, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(22064.5391, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(28146.8984, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(27153.6738, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(25283.6641, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(24649.5020, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(26744.0762, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(30532.8008, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(28145.0469, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(34086.1133, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(35758.7852, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(33594.1953, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(31736.0371, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(36420.2031, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(36873.0195, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(36177.7227, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(36069.4844, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(36333.8828, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(40336.1094, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(49424.8281, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -18.0\n","loss:  tensor(44504.3672, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(52668.8203, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(51216.0508, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(64293.8203, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(60776.8164, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(56947.9961, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(56498.7344, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(70313.8906, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(68104.2500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(61401.6523, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(75187.3672, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(78563.5312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(87471.4922, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(80292.0859, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(88896.4453, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(103482.8047, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(95172.4844, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(92264.7891, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(99964.3906, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(94832.8984, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(94732.4297, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(95100.9297, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(109321.4688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(102351.0781, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(101883.6641, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(119545.2422, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(110466.2188, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(111023.5156, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(121089.1094, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(106983.5078, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(122737.5469, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(114296.7344, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(106335.7188, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(124575.3672, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(118023.1953, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(108645.2891, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(126686.5781, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(126912.9453, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(107795.2656, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(125528.1094, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(120314.9609, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(129192.7578, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(138607.4219, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(132542.4219, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(124861., device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(134550.6719, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(134931.6719, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(131582.4219, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(129088.8203, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(142650.0312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(141172.5625, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(156392.9375, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(154254.9219, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(169115.2969, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(182013.7344, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(182537.5469, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(190924.0781, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(188613.7656, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(200153.3125, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(203688.2344, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(212261.1250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(206009.0156, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(220528.2500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(209867.9375, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(206398.9062, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(206656.7656, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(206437.3125, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(200292.3906, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(186449.8594, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(176507.8906, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(171010.5469, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(207763.2500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(189343.7656, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(205775.6719, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(207929.7656, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(201247.9688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(207561.8750, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(212588.9062, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(208197.8906, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(185989.0781, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(208534.5000, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(202954.6719, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(217820.1406, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(208530.6875, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(227397.0312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(221737., device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(214507.1250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(237324.7031, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(277206.0938, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(241995.7188, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(302806.4062, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(317177.8750, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(307174.5938, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(334749.0938, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(319743., device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(343221.7812, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(413308.5312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(385955.7500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(404515.1875, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(409777.5000, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(389567.6250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(383751.6562, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(413087.7500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(380286.0312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(426929.9375, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(406589.2812, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(388921.2188, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(402658.4688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(400098.5312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(422425.5938, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(398433.5625, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(369888.7812, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(383782.1562, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(388906.6250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(375708.7500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(344200.1250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(342603.1875, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(374609.6562, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(361411.5000, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(364454.6562, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(351315.1250, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(354229.8125, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(416587.2500, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(404004.5312, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -20.0\n","loss:  tensor(355877.4062, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(386815.2812, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(353870.5000, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(339016.3438, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(349349.8750, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(399088.6562, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(385465.4688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(345218.1875, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(378224.3438, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(422617.7188, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -21.0\n","loss:  tensor(416834.9688, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -18.0\n","loss:  tensor(399447.5000, device='cuda:0', grad_fn=<DivBackward0>)\n","episode done, reward:  -19.0\n","loss:  tensor(431897.7812, device='cuda:0', grad_fn=<DivBackward0>)\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-34c68a9954a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_assault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/multi-game-agent/src/train_assault.py\u001b[0m in \u001b[0;36miterate_train\u001b[0;34m(num_episodes)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;31m#sample random transitions calculate loss and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mTRANSITIONS_BATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mupdate_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/multi-game-agent/src/train_assault.py\u001b[0m in \u001b[0;36mupdate_net\u001b[0;34m(dqn_policy, dqn_target, optimizer)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mhuber_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdqn_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"UNix1M4hdUgL","executionInfo":{"status":"aborted","timestamp":1610560769197,"user_tz":-120,"elapsed":5034875,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":["show_video()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYs-LUOPMqZ6","executionInfo":{"status":"aborted","timestamp":1610560769200,"user_tz":-120,"elapsed":5034871,"user":{"displayName":"Николай Пашов","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIksI2BKEi-Jz8c_r3Fe8I6l22oLNWlE76_1l8KQ=s64","userId":"08896561681039562531"}}},"source":[""],"execution_count":null,"outputs":[]}]}